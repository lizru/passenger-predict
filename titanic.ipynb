{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "695fbe06",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction\n",
    "This analysis aims to build a machine learning model to predict the survival of passengers aboar the Titanic using various attributes such as class, age, and sex. The dataset is available through Kaggle and contains information on 891 passengers. This notebook includes preprocessing data, handling missing values, and relevant feature engineering. A Random Forest Classifier will be used as a baseline model to predict survival."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546835e7",
   "metadata": {},
   "source": [
    "## 1. Dependencies & Data Loading\n",
    "The dataset for this analysis was obtained from Kaggle. Link: https://www.kaggle.com/datasets/yasserh/titanic-dataset?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7f0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,  classification_report\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d58df4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into pandas\n",
    "df = pd.read_csv(\"Titanic-Dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf88f44",
   "metadata": {},
   "source": [
    "## 2. Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97990a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming Parent/Child column 'ParCh' for consistency with Sibling/Spouse column 'SibSp'\n",
    "df.rename(columns={'Parch': 'ParCh'}, inplace=True)\n",
    "\n",
    "# Inspecting the data\n",
    "print(df.head(),\"\\n\")\n",
    "print(\"Shape: \", df.shape)\n",
    "print(\"Columns: \", df.columns)\n",
    "print(\"Info: \", df.info())\n",
    "print(\"Describe: \", df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc19372",
   "metadata": {},
   "source": [
    "From the description of the dataset above, we can see that it has the following 12 columns:\n",
    "- PassengerId: integer index, a unique key for each passenger\n",
    "- Survived: binary feature marking survival\n",
    "- Pclass: numerical marking class \n",
    "- Name: Last, first. First name includes title, can include alternative name in quotes or parentheses\n",
    "- Sex: male or female\n",
    "- Age: int\n",
    "- SibSp: counts siblings or spouse aboard\n",
    "- Parch: counts parents or children aboard\n",
    "- Ticket: ticket number\n",
    "- Fare: numerical passenger fare\n",
    "- Cabin: cabin number\n",
    "- Embarked: embark port (C = Cherbourg, Q = Queenstown, S = Southampton)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c7d706",
   "metadata": {},
   "source": [
    "## 3. Handling Missing Values & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e97f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "print(\"Missing value counts, from 891 rows:\\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295ca702",
   "metadata": {},
   "source": [
    "Missing values need to be handled in the columns Age (177/891 missing), Cabin (687/891 missing), and Embarked (2/891 missing).\n",
    "- Embarked has only 2 values missing.\n",
    "- Cabin is missing for the majority of records.\n",
    "- Age is missing for about 1/5 records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a40f337",
   "metadata": {},
   "source": [
    "### 3.1 Handling 'Embarked'\n",
    "Because there are not many records, missing values can be resolved by dropping rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c69dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Embarked'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce4802b",
   "metadata": {},
   "source": [
    "### 3.2 Handling 'Cabin' and Creating 'Deck' and 'HasCabin'\n",
    "\n",
    "Handling missing cabin values is more complex, as the majority are missing. In order to gain insight, two features are created: Deck and HasCabin. Before creating these features, we are able to impute a small number of deck values by looking at ticket groups. Many tickets come with mulitple passengers in the same cabin or close cabins, so we can infer that passengers who are missing a cabin value but have a shared ticket are placed in a similar cabin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ab401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print information about passengers on the same ticket\n",
    "ticket_counts = df['Ticket'].value_counts()\n",
    "for ticket, count in ticket_counts[ticket_counts > 1].items():\n",
    "    print(f\"\\nTicket: {ticket}, Count: {count}\")\n",
    "    print(df[df['Ticket'] == ticket][['Name', 'Cabin', 'Pclass', 'SibSp', 'ParCh']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65d7c85",
   "metadata": {},
   "source": [
    "For the cabin data present, we can focus on extracting the deck level, because it is likely to have an impact on survival. Deck names follow a format of a letter followed by digits, so regex is used to extract the first character to save the deck the passenger was on. Passengers who do not have a cabin will be assigned the deck 'U', representing undefined.\n",
    "\n",
    "The majority of cabin values will still be missing. To mark this, a new binary feature, HasCabin, is created. \n",
    "\n",
    "Because deck is likely to be more relevant than a specific cabin number, the original feature Cabin is dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f809c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new feature, deck, based on cabin\n",
    "df['Deck'] = df['Cabin'].apply(lambda x: x[0] if pd.notnull(x) else np.nan)\n",
    "\n",
    "# finds the most common deck for each ticket group, if data exists\n",
    "deck_mode = df.groupby('Ticket')['Deck'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
    "\n",
    "\n",
    "def infer_deck(row):\n",
    "    # If deck exists, keep it\n",
    "    if pd.notnull(row['Deck']):\n",
    "        return row['Deck']\n",
    "    # Else, if data is available, fill with mode deck of the ticket group\n",
    "    return deck_mode.get(row['Ticket'], np.nan)\n",
    "\n",
    "# Run deck imputation, create binary cabin feature\n",
    "df['Deck'] = df.apply(infer_deck, axis=1)\n",
    "df['HasCabin'] = df['Deck'].notnull().astype(int)\n",
    "df['Deck'] = df['Cabin'].apply(lambda x: x[0] if pd.notnull(x) else 'U')\n",
    "\n",
    "# Drops original cabin\n",
    "df = df.drop('Cabin', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b68ad07",
   "metadata": {},
   "source": [
    "### 3.3 Extracting 'Title' for Imputation & Modeling\n",
    "\n",
    "We extract the passenger's title (e.g., Mr, Miss, Dr) from the name field using regular expression. \n",
    "This feature is useful both as a predictor for age (younger titles like \"Master\" or \"Miss\" imply lower age) and later as a feature for survival prediction.\n",
    "Less common titles are grouped as \"Other\" to reduce noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2ef3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates title feature\n",
    "df['Title'] = df['Name'].str.extract(r',\\s*([^\\.]+)\\.', expand=False)\n",
    "\n",
    "# Labels rare titles as \"Other\" to simplify noise\n",
    "title_counts = df['Title'].value_counts()\n",
    "rare_titles = title_counts[title_counts < 10].index\n",
    "df['Title'] = df['Title'].replace(rare_titles, 'Other')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c206d0bf",
   "metadata": {},
   "source": [
    "### 3.4 Imputing Age with Random Forest\n",
    "\n",
    "Roughly 20% of the age values are missing, and there is correlation between age and other features in the dataset. To address this, imputation is used to fill in the missing age values.\n",
    "\n",
    "A Random Forest model was selected to impute age. Random Forest works for this task because it can capture complex relationships between features, which a simple imputation method cannot do. It is flexible, can handle both numeric and categorical variables, and can consider multiple features simultaneously to generate more reliable age predictions. Additionally, it remains fairly resistant to overfitting. Although Random Forest models can be computationally expensive, the small size of this dataset means efficiency is not significantly impacted. Alternatively, median-based imputation through groups could provide reasonably accurate imputation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5721b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exploring correlations between age & numeric features\n",
    "numeric_vars = ['Age', 'Fare', 'SibSp', 'ParCh']\n",
    "corr_matrix = df[numeric_vars].corr()\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Heatmap')\n",
    "print(\"Correlation Matrix: \", corr_matrix)\n",
    "plt.show()\n",
    "\n",
    "# Exploring correlations between age & categorical features\n",
    "categorical_vars = ['Sex', 'Pclass', 'Embarked', 'Title']\n",
    "for category in categorical_vars:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.boxplot(x=category, y='Age', data=df)\n",
    "    plt.title(f'Age distribution by {category}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model for Age Imputation\n",
    "\n",
    "# Uses label encoding on title so it can be included\n",
    "le_title = LabelEncoder()\n",
    "df['EncodedTitle'] = le_title.fit_transform(df['Title'])\n",
    "\n",
    "# features we will be using in the model\n",
    "age_features = ['SibSp', 'ParCh', 'Pclass', 'EncodedTitle']\n",
    "\n",
    "# filter where data for age is missing\n",
    "age_known = df[df['Age'].notnull()]\n",
    "age_unknown = df[df['Age'].isnull()]\n",
    "\n",
    "X = age_known[age_features]\n",
    "y = age_known['Age']\n",
    "\n",
    "\n",
    "# first training the model on 80% of the known data to evaluate effectiveness\n",
    "X_train_eval, X_test_eval, y_train_eval, y_test_eval = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_eval = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_eval.fit(X_train_eval, y_train_eval)\n",
    "y_pred_eval = rf_eval.predict(X_test_eval)\n",
    "\n",
    "print(\"R^2: \", r2_score(y_test_eval, y_pred_eval))\n",
    "print(\"MSE: \", mean_squared_error(y_test_eval, y_pred_eval))\n",
    "\n",
    "\n",
    "\n",
    "# making the final imputation model on all known data\n",
    "rf_final = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_final.fit(X, y)\n",
    "\n",
    "\n",
    "# assign features to model\n",
    "df.loc[df['Age'].isnull(), 'Age'] = rf_final.predict(age_unknown[age_features])\n",
    "\n",
    "\n",
    "# graphing importance of features in model\n",
    "feature_importance = rf_final.feature_importances_\n",
    "\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "sorted_importances = feature_importance[sorted_idx]\n",
    "sorted_features = [age_features[i] for i in sorted_idx]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.barh(sorted_features, sorted_importances)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance for Age Imputation')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plotting predicted values against perfect performance\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.scatterplot(x=y_test_eval, y=y_pred_eval, alpha=0.6)\n",
    "plt.plot([0, 80], [0, 80], color='red', linestyle='--') \n",
    "plt.xlabel('Actual Age')\n",
    "plt.ylabel('Predicted Age')\n",
    "plt.title('Predicted vs Actual Age (Random Forest)')\n",
    "plt.xlim(0, 80)\n",
    "plt.ylim(0, 80)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea18f169",
   "metadata": {},
   "source": [
    "The Random Forest regressor model was trained on relevant features and has an R-squared score of approximately .46 & a mean squred error of 130.6. This indicates that the model accounts for almost half of the variance in passenger ages, which is acceptable due to the noise and missing data. While other imputation methods like median group-based imputation can provide baseline estimates, the Random Forest model can capture interactions among multiple features and can provide a more robust approach towards missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e9395",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34422d5",
   "metadata": {},
   "source": [
    "### 4.1 Label Encoding for Categorical Features\n",
    "Categorical features that could affect our model must be encoded. Because the model will be tree-based, label encoding is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc6268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "le_deck = LabelEncoder()\n",
    "df['EncodedDeck'] = le_deck.fit_transform(df['Deck'])\n",
    "\n",
    "le_sex = LabelEncoder()\n",
    "df['EncodedSex'] = le_sex.fit_transform(df['Sex'])\n",
    "\n",
    "le_embarked = LabelEncoder()\n",
    "df['EncodedEmbarked'] = le_embarked.fit_transform(df['Embarked'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c7d832",
   "metadata": {},
   "source": [
    "### 4.2 Interaction Features\n",
    "Features capturing interactions between variables like age, sex, and title can provide nuance and insight into underlying relations in the data. For example, SexByAge could capture different survival rates among age groups for each sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d827ddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age and title interaction\n",
    "df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 13, 18, 36, 60, 100], labels=False)\n",
    "df['TitleByAgeGroup'] = df['EncodedTitle'] * df['AgeGroup']\n",
    "\n",
    "# Sex and age interaction\n",
    "df['SexByAge'] = df['EncodedSex'] * df['Age']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77041b2c",
   "metadata": {},
   "source": [
    "### 4.3 Family-Based Features\n",
    "Creating features from Age, SibSp, and ParCh provides insight into whether someone is traveling in a group. The feature FamilySize is created based on SibSp and ParCh (along with the individual), which is then used to create a more simple binary feature IsAlone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e16a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Child'] = df['Age'].apply(lambda x: 1 if x < 13 else 0)\n",
    "df['FamilySize'] = df['SibSp'] + df['ParCh'] + 1\n",
    "df['IsAlone'] = df['FamilySize'].apply(lambda x:1 if x == 1 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11e1ef6",
   "metadata": {},
   "source": [
    "## 5. Data Exploration on Survival\n",
    "\n",
    "This section explores correlations between different features and overall survival.\n",
    "\n",
    "Sex, age, and deck will likely have significant impacts as they determine lifeboat prioritization & accessiblity. A new feature 'child' is created to capture distinction in age. \n",
    "Having family on board also affects survival chances. The feature FamilySize is created based on SibSp and ParCh (along with the individual), along with a more simple binary feature IsAlone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba48f352",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plotting effect of family size on survival, visual weight representing frequency\n",
    "counts = df['FamilySize'].value_counts().sort_index()\n",
    "normalized_counts = (counts - counts.min()) / (counts.max() - counts.min())\n",
    "palette = sns.light_palette(\"navy\", reverse=True, n_colors=len(normalized_counts))\n",
    "color_map = {size: palette[i] for i, size in enumerate(normalized_counts.index)}\n",
    "sns.barplot(x='FamilySize', y='Survived', data=df, errorbar=None, hue='FamilySize', palette=color_map, dodge=False)\n",
    "plt.legend([],[],frameon=False) # remove legend\n",
    "plt.title('Survival Rate by Family Size (1 = Alone)')\n",
    "plt.show()\n",
    "\n",
    "# Plotting survival by sex\n",
    "sns.barplot(x='Sex', y='Survived', data=df)\n",
    "plt.title('Survival Rate by Sex')\n",
    "plt.show()\n",
    "\n",
    "# Survival by Deck\n",
    "sns.barplot(x='Deck', y='Survived', data=df, order=sorted(df['Deck'].unique()))\n",
    "plt.title('Survival Rate by Deck')\n",
    "plt.show()\n",
    "\n",
    "# Survival by Class\n",
    "sns.barplot(x='Pclass', y='Survived', data=df)\n",
    "plt.title('Survival Rate by Passenger Class')\n",
    "plt.show()\n",
    "\n",
    "# Survival by Title\n",
    "sns.barplot(x='Title', y='Survived', data=df)\n",
    "plt.title('Survival Rate by Title')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Survival by Age\n",
    "sns.boxplot(x='Survived', y='Age', data=df)\n",
    "plt.title('Age Distribution by Survival')\n",
    "plt.show()\n",
    "\n",
    "# Survival by Child \n",
    "sns.barplot(x='Child', y='Survived', data=df)\n",
    "plt.title('Survival Rate by Child Status')\n",
    "plt.show()\n",
    "\n",
    "# Survival of Alone\n",
    "sns.barplot(x='IsAlone', y='Survived', data=df)\n",
    "plt.title('Survival Rate by Traveling Alone')\n",
    "plt.show()\n",
    "\n",
    "df['LogFare'] = np.log1p(df['Fare'])\n",
    "# Survival by Fare\n",
    "sns.boxplot(x='Survived', y='LogFare', data=df)\n",
    "plt.title('Fare Distribution by Survival')\n",
    "plt.show()\n",
    "\n",
    "# Correlation Heatmap for numeric features\n",
    "numeric_features = ['Survived', 'Age', 'Fare', 'FamilySize', 'IsAlone', 'Child', 'EncodedSex', 'EncodedEmbarked', 'EncodedTitle', 'EncodedDeck']\n",
    "corr_matrix = df[numeric_features].corr()\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", square=True)\n",
    "plt.title('Correlation Heatmap of Numeric Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a0c4f8",
   "metadata": {},
   "source": [
    "The plots show that sex and age are strong predictors of survival. Whether or not an individual is alone also has a moderate effect. \n",
    "\n",
    "A random forest classifier model will be used to predict survival.\n",
    "Additional combination features (ie, SexByAge) can be used to boost model accuracy by finding nuanced interactions between variables. Less relevant features that add noise can be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43824547",
   "metadata": {},
   "source": [
    "## 6. Survival Prediction Model with Random Forest\n",
    "\n",
    "### 6.1 Fitting Model\n",
    "Given the Titanic datasetâ€™s mix of categorical and continuous variables, relatively small size, and complex underlying relationships, Random Forest provides a strong baseline model for survival prediction with good balance between accuracy and interpretability. It works by building several decision treees and combining their results to get more accurate and stable predictions, while being less likely to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536664a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "# feature selection\n",
    "features = ['Pclass', 'EncodedSex', 'Age', 'Fare', 'IsAlone', 'Child', 'EncodedEmbarked', 'EncodedTitle', 'EncodedDeck', 'HasCabin', 'SexByAge', 'TitleByAgeGroup']\n",
    "target = 'Survived'\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "\n",
    "# 20% of the data is saved for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# create model\n",
    "clf = RandomForestClassifier(n_estimators=75, random_state=42)\n",
    "\n",
    "# cross validation on the training set\n",
    "cv_scores = cross_val_score(clf, X, y, cv=10)\n",
    "print(f\"Mean CV Accuracy: {cv_scores.mean()}\")\n",
    "\n",
    "# fit model to training set\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# evaluate model on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test Set Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a29935",
   "metadata": {},
   "source": [
    "### 6.2 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1b1240",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluation metrics on test set\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# plots confusion matrix\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues', xticklabels=['Died', 'Survived'], yticklabels=['Died', 'Survived'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plots feature importance\n",
    "feature_importance = clf.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "sorted_importances = feature_importance[sorted_idx]\n",
    "sorted_features = [features[i] for i in sorted_idx]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.barh(sorted_features, sorted_importances)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance for Model')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aefd24c",
   "metadata": {},
   "source": [
    "## 7. Conclusion & Potential Improvement\n",
    "The Random Forest Classifier created above provides a baseline for predicting Titanic survival, but there is room for improvement. Cross validation with 10 folds was used to achieve an accuracy of approximately 81.7%, with the hold-out test achieving 81.4% accuracy.  The classification report tells us that this model is slightly weaker at predicting survivors, and sometimes incorrectly predicts survivors where there are none. The precision score for Survived tells us that only 74% of the passengers predicted to survive were in fact survivors, implying room for finer tuning. The recall for survivors tells us that 81% of the predicted survivors did survive, but there are some false negatives. The F1 scores, which balances precision and recall, are 84% for non-survivors and 77% for survivors More precise feature engineering to consider nuanced interactions between variables could boost accuracy. Fine-tuning the hyperparameters of the model could also improve it. Further, using a gradient boosting method such as an XGBoost model would likely provide an increase in accuracy in exchange for more precise tuning. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
